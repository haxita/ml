{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "\n",
    "This material, no matter whether in printed or electronic form,\n",
    "may be used for personal and non-commercial educational use only.\n",
    "Any reproduction of this manuscript,\n",
    "no matter whether as a whole or in parts,\n",
    "no matter whether in printed or in electronic form,\n",
    "requires explicit prior acceptance of the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- Assignment 7 - SS 2023 -->\n",
    "\n",
    "# Normalising Flows (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook contains one of the assignments for the exercises in Deep Learning and Neural Nets 2.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility functions that should work without (too much) problems.\n",
    "Please, do not alter this code or add extra import statements in your submission, unless explicitly allowed!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this assignment, we take a look at the change of variables formula\n",
    "for distributions and how it can be used to train a generative model.\n",
    "These generative models are known as normalising flows,\n",
    "and make it possible to do density estimation with neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import distributions\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "torch.manual_seed(1806)\n",
    "torch.cuda.manual_seed(1806)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# google colab data management\n",
    "import os.path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    _home = 'gdrive/MyDrive/'\n",
    "except ImportError:\n",
    "    _home = '~'\n",
    "finally:\n",
    "    data_root = os.path.join(_home, '.pytorch')\n",
    "\n",
    "print(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_to_image(*data: torch.Tensor,\n",
    "                  means: tuple = (0, ), stds: tuple = (1., )) -> Image:\n",
    "    \"\"\"\n",
    "    Convert multiple tensors to one big image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data0, data1, ... dataN : torch.Tensor\n",
    "        One or more tensors to be merged into a single image.\n",
    "    means : tuple or torch.Tensor, optional\n",
    "        Original mean of the image before normalisation.\n",
    "    stds : tuple or torch.Tensor, optional\n",
    "        Original standard deviation of the image before normalisation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    image : Image\n",
    "        PIL image with all of the tensors next to each other.\n",
    "    \"\"\"\n",
    "    # concatenate all data\n",
    "    big_pic = torch.cat([x for x in data], dim=-1)\n",
    "\n",
    "    means = torch.tensor(means)\n",
    "    stds = torch.tensor(stds)\n",
    "    to_image = transforms.Compose([\n",
    "        # inverts normalisation of image\n",
    "        transforms.Normalize(-means / stds, 1. / stds),\n",
    "        transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "        transforms.ToPILImage()\n",
    "    ])\n",
    "\n",
    "    return to_image(big_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NormalisingFlowTrainer:\n",
    "    \n",
    "    def __init__(self,\n",
    "        model: nn.Module,\n",
    "        criterion: nn.Module,\n",
    "        optimiser: optim.Optimizer,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : torch.nn.Module\n",
    "            Neural Network that will be trained.\n",
    "        criterion : torch.nn.Module\n",
    "            Loss function to use for training.\n",
    "        optimiser : torch.optim.Optimizer\n",
    "            Optimisation strategy for training.\n",
    "        tracker : Tracker, optional\n",
    "            Tracker to keep track of training progress.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimiser = optimiser\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\" Current state of learning. \"\"\"\n",
    "        return {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"objective\": self.criterion.state_dict(),\n",
    "            \"optimiser\": self.optimiser.state_dict(),\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict: dict):\n",
    "        \"\"\" Restore learning state. \"\"\"\n",
    "        self.model.load_state_dict(state_dict[\"model\"])\n",
    "        self.criterion.load_state_dict(state_dict[\"objective\"])\n",
    "        self.optimiser.load_state_dict(state_dict[\"optimiser\"])\n",
    "        \n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\" Device of the (first) model parameters. \"\"\"\n",
    "        return next(self.model.parameters()).device\n",
    "\n",
    "    def _forward(self, data: DataLoader, metric: callable):\n",
    "        \"\"\" Unsupervised variation on forward propagation. \"\"\"\n",
    "        device = self.device\n",
    "\n",
    "        for x, _ in data:\n",
    "            x = x.to(device)\n",
    "            outputs = self.model(x)\n",
    "            res = metric(outputs)\n",
    "            yield res\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, data: DataLoader, metric: callable) -> list:\n",
    "        self.model.eval()\n",
    "\n",
    "        results = self._forward(data, metric)\n",
    "        return [res.item() for res in results]\n",
    "\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def update(self, data: DataLoader) -> list:\n",
    "        self.model.train()\n",
    "\n",
    "        errs = []\n",
    "        for err in self._forward(data, self.criterion):\n",
    "            errs.append(err.item())\n",
    "\n",
    "            self.optimiser.zero_grad()\n",
    "            err.backward()\n",
    "            self.optimiser.step()\n",
    "\n",
    "        return errs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def display_inverted(self, y: torch.Tensor, \n",
    "                         prior : distributions.Distribution = None):\n",
    "        \"\"\"\n",
    "        Display data using the inverse function of a normalising flow.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        flow : nn.Module\n",
    "            The normalising flow.\n",
    "        y : torch.Tensor\n",
    "            A sample batch from the easy distribution to visualise.\n",
    "        prior : Distribution, optional\n",
    "            The prior distribution for printing the\n",
    "            probability of the generated images.\n",
    "        \"\"\"\n",
    "        x = self.model.invert(y)\n",
    "        if prior is not None:\n",
    "            log_py = prior.log_prob(y).view(len(x), -1).sum(-1)\n",
    "            log_det = flow.log_det_jacobi(x)\n",
    "            log_px = log_py + log_det\n",
    "            print(f\" --- first image info --- \", end='')\n",
    "            print(f\"log-py: {log_py[0]:.1e}, log-px: {log_px[0]:.1e}\")\n",
    "\n",
    "        x_vis = x.view(-1, 1, 28, 28).cpu()\n",
    "        x_im = data_to_image(*x_vis, means=(.1307, ), stds=(.3081, ))\n",
    "        display(x_im, metadata={'width': '100%'})\n",
    "    \n",
    "    def train(self, loader: DataLoader, num_epochs: int = 10, vis_every: int = 5,\n",
    "              prior: distributions.Distribution = None):\n",
    "        \"\"\"\n",
    "        Train a normalising flow for a number of epochs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        loader : DataLoader\n",
    "            A data loader for iterating over batches of the data.\n",
    "        num_epochs : int, optional\n",
    "            Number of times to iterate the dataset.\n",
    "        vis_every : int, optional\n",
    "            Frequency, during training, of \n",
    "            intermediate visualisation of generated samples.\n",
    "        prior : Distribution, optional\n",
    "            The prior distribution for the normalising flow (for visualisation).\n",
    "        \"\"\"\n",
    "        dev = self.device\n",
    "        if prior is None:\n",
    "            try:\n",
    "                prior = self.criterion.prior\n",
    "            except AttributeError:\n",
    "                x, _ = next(iter(loader))\n",
    "                mean = torch.zeros_like(x[0]).to(dev)\n",
    "                std = torch.ones_like(x[0]).to(dev)\n",
    "                prior = distributions.Normal(mean, std)\n",
    "\n",
    "        y_vis = prior.sample((10, )).to(dev)\n",
    "\n",
    "        errs = self.evaluate(loader, self.criterion)\n",
    "        print(f\"Epoch {0:02d} - avg loss: {sum(errs) / len(errs)}\")\n",
    "        self.display_inverted(y_vis, prior)\n",
    "\n",
    "        for i in range(1, num_epochs + 1):\n",
    "            errs = self.update(loader)\n",
    "            print(f\"Epoch {i:02d} - avg loss: {sum(errs) / len(errs)}\")\n",
    "            if i % vis_every == 0:\n",
    "                self.display_inverted(y_vis, prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Functions and Distributions\n",
    "\n",
    "One of the foundations of normalising flows is the *change of variable* formula,\n",
    "which allows to reason about the effect functions have on distributions.\n",
    "Given a random variable $Y = f(X)$, where $f$ is a bijective function\n",
    "and $X$ is a random variable from a distribution with pdf $p_X$,\n",
    "the change of variable allows us to write down the pdf, $p_Y$,\n",
    "that defines the distribution of $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For a scalar function $f : \\mathbb{R} \\to \\mathbb{R}$,\n",
    "the change of variable formula is given as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  p_Y(y) = p_X(f^{-1}(y)) \\cdot \\left|\\frac{\\partial f^{-1}(y)}{\\partial y}\\right|.\n",
    "\\end{aligned}$$\n",
    "\n",
    "This formula reveals that the probability density of $Y = f(X)$\n",
    "is proportional to the probability density of $X$.\n",
    "Moreover, the proportion of densities is the gradient of the inverse,\n",
    "or, equivalently, the inverse of the gradient, $f'(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To gain some intuition for the change of variable formula,\n",
    "it might help to consider the visualisation of the histogram below.\n",
    "Consider inputs distributed according to $X \\sim \\mathcal{U}(0, 2)$,\n",
    "which is illustrated by the histogram below the $x$-axis.\n",
    "To get a feeling for the distribution of $Y = X^2$,\n",
    "we map each bin in the histogram to its corresponding range on the $y$-axis.\n",
    "\n",
    "The *probability densities* are the height of each bin.\n",
    "The area of each bin represents the actual probability,\n",
    "and must be conserved through function application.\n",
    "This means that if a bin on the $x$-axis is mapped to a smaller or larger range,\n",
    "the density on the $y$-axis will proportionally grow or shrink, respecively.\n",
    "Therefore, the change in probability density is proportional\n",
    "to the the change of range due to the function.\n",
    "For infinitesimally small bins, this \"change of range\" \n",
    "eventually corresponds to the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<figure>\n",
    "  <figcaption style=\"text-align: center\"> Change of Variables through the function $f(x) = x^2$ </figcaption>\n",
    "  <figure style=\"display: inline-block; max-width: 33%; margin: 0;\">\n",
    "    <img alt=\"visualisation of change of variables formula with one histogram bin\" src=\"data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22utf-8%22%3F%3E%0A%3Csvg%20viewBox%3D%220%200%20500%20500%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%20%20%3Cg%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22200%22%20y1%3D%22320%22%20x2%3D%22200%22%20y2%3D%22300%22%2F%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22180%22%20y1%3D%22300%22%20x2%3D%22200%22%20y2%3D%22300%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%20%20%3Crect%20x%3D%22200%22%20y%3D%22320%22%20width%3D%22100%22%20height%3D%2260%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Cg%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22300%22%20y1%3D%22320%22%20x2%3D%22300%22%20y2%3D%22100%22%2F%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22180%22%20y1%3D%22100%22%20x2%3D%22300%22%20y2%3D%22100%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%20%20%3Crect%20x%3D%22150%22%20y%3D%22100%22%20width%3D%2230%22%20height%3D%22200%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Cpath%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20fill%3A%20none%3B%22%20d%3D%22M%20200%20300%20C%20233.33%20300%20266.67%20233.33%20300%20100%22%2F%3E%0A%20%20%3Cg%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%22%20x1%3D%22200%22%20y1%3D%2290%22%20x2%3D%22200%22%20y2%3D%22310%22%2F%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%22%20x1%3D%22190%22%20y1%3D%22300%22%20x2%3D%22410%22%20y2%3D%22300%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%3C%2Fsvg%3E\" style=\"width: 500px\" />\n",
    "    <figcaption style=\"text-align: center\">(a) 1-bin histogram</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; max-width: 33%; margin: 0;\">\n",
    "    <img alt=\"visualisation of change of variables formula with two histogram bins\" src=\"data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22utf-8%22%3F%3E%0A%3Csvg%20viewBox%3D%220%200%20500%20500%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%20%20%3Cg%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22200%22%20y1%3D%22320%22%20x2%3D%22200%22%20y2%3D%22300%22%2F%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22180%22%20y1%3D%22300%22%20x2%3D%22200%22%20y2%3D%22300%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%20%20%3Crect%20x%3D%22200%22%20y%3D%22320%22%20width%3D%2250%22%20height%3D%2260%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Cg%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22250%22%20y1%3D%22320%22%20x2%3D%22250%22%20y2%3D%22250%22%2F%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22180%22%20y1%3D%22250%22%20x2%3D%22250%22%20y2%3D%22250%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%20%20%3Crect%20x%3D%22120%22%20y%3D%22250%22%20width%3D%2260%22%20height%3D%2250%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Crect%20x%3D%22249.96%22%20y%3D%22320%22%20width%3D%2250%22%20height%3D%2260%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Cg%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22300%22%20y1%3D%22320%22%20x2%3D%22300%22%20y2%3D%22100%22%2F%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22180%22%20y1%3D%22100%22%20x2%3D%22300%22%20y2%3D%22100%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%20%20%3Crect%20x%3D%22160%22%20y%3D%22100%22%20width%3D%2220%22%20height%3D%22150%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Cpath%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20fill%3A%20none%3B%22%20d%3D%22M%20200%20300%20C%20233.33%20300%20266.67%20233.33%20300%20100%22%2F%3E%0A%20%20%3Cg%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%22%20x1%3D%22200%22%20y1%3D%22310%22%20x2%3D%22200%22%20y2%3D%2290%22%2F%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%22%20x1%3D%22190%22%20y1%3D%22300%22%20x2%3D%22410%22%20y2%3D%22300%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%3C%2Fsvg%3E\" style=\"width: 500px\" />\n",
    "    <figcaption style=\"text-align: center\">(b) 2-bin histogram</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; max-width: 33%; margin: 0;\">\n",
    "    <img alt=\"visualisation of change of variables formula with four histogram bins\" src=\"data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22utf-8%22%3F%3E%0A%3Csvg%20viewBox%3D%220%200%20500%20500%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%20%20%3Cg%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22200%22%20y1%3D%22320%22%20x2%3D%22200%22%20y2%3D%22300%22%2F%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22180%22%20y1%3D%22300%22%20x2%3D%22200%22%20y2%3D%22300%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%20%20%3Crect%20x%3D%22200.04%22%20y%3D%22320%22%20width%3D%2225%22%20height%3D%2260%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Cg%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22225%22%20y1%3D%22320%22%20x2%3D%22225%22%20y2%3D%22287.5%22%2F%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22180%22%20y1%3D%22287.5%22%20x2%3D%22225%22%20y2%3D%22287.5%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%20%20%3Crect%20x%3D%2260%22%20y%3D%22287.5%22%20width%3D%22120%22%20height%3D%2212.5%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Crect%20x%3D%22225%22%20y%3D%22320%22%20width%3D%2225%22%20height%3D%2260%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Cg%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22250%22%20y1%3D%22320%22%20x2%3D%22250%22%20y2%3D%22250%22%2F%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22180%22%20y1%3D%22250%22%20x2%3D%22250%22%20y2%3D%22250%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%20%20%3Crect%20x%3D%22140%22%20y%3D%22250%22%20width%3D%2240%22%20height%3D%2237.5%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Crect%20x%3D%22250%22%20y%3D%22320%22%20width%3D%2225%22%20height%3D%2260%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Cg%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22275%22%20y1%3D%22320%22%20x2%3D%22275%22%20y2%3D%22187.5%22%2F%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22180%22%20y1%3D%22187.5%22%20x2%3D%22275%22%20y2%3D%22187.5%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%20%20%3Crect%20x%3D%22156%22%20y%3D%22187.5%22%20width%3D%2224%22%20height%3D%2262.5%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Crect%20x%3D%22275%22%20y%3D%22320%22%20width%3D%2225%22%20height%3D%2260%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Cg%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22300%22%20y1%3D%22320%22%20x2%3D%22300%22%20y2%3D%22100%22%2F%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20stroke-dasharray%3A%205px%3B%22%20x1%3D%22180%22%20y1%3D%22100%22%20x2%3D%22300%22%20y2%3D%22100%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%20%20%3Crect%20x%3D%22162.9%22%20y%3D%22100%22%20width%3D%2217.14%22%20height%3D%2287.5%22%20style%3D%22stroke%3A%20rgb%28255%2C%20255%2C%20255%29%3B%20fill%3A%20rgb%2831%2C%20119%2C%20180%29%3B%22%2F%3E%0A%20%20%3Cpath%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%20fill%3A%20none%3B%22%20d%3D%22M%20200%20300%20C%20233.33%20300%20266.67%20233.33%20300%20100%22%2F%3E%0A%20%20%3Cg%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%22%20x1%3D%22200%22%20y1%3D%22310%22%20x2%3D%22200%22%20y2%3D%2290%22%2F%3E%0A%20%20%20%20%3Cline%20style%3D%22stroke%3A%20rgb%280%2C%200%2C%200%29%3B%22%20x1%3D%22190%22%20y1%3D%22300%22%20x2%3D%22410%22%20y2%3D%22300%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%3C%2Fsvg%3E\" style=\"width: 500px\" />\n",
    "    <figcaption style=\"text-align: center\">(c) 4-bin histogram</figcaption>\n",
    "  </figure>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For multi-variable functions, we simply exchange the derivative\n",
    "by the determinant of the Jacobian, which gives:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  p_Y(\\mathbf{y}) = p_X(g^{-1}(\\mathbf{y})) \\cdot \\left|\\det \\mathcal{J}_g(\\mathbf{y})\\right|^{-1}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Note that the determinant of the Jacobian corresponds to \n",
    "the change of area due to function application.\n",
    "To gain some intuition for this version of the formula,\n",
    "the visualisations above can be generalised to e.g. 2d histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 1:  Jacobian Check (3 points)\n",
    "\n",
    "To get some feeling for the change of variable formula,\n",
    "an invertible function with easy-to-compute Jacobian determinant has been provided.\n",
    "To make sure that there are no mistakes in the provided module,\n",
    "you can use the `autograd` magic in pytorch to verify its implementation.\n",
    "\n",
    " > Use the automatic differentiation engine of pytorch\n",
    " > to compute the Jacobian of a pytorch module.\n",
    " > The `jacobian` function should compute the Jacobian matrix\n",
    " > of a given module at a given input.\n",
    " > It should work for any PyTorch module, also if inputs and outputs are different!\n",
    " > You should **not** use `torch.autograd.jacobian` (or similar functions)\n",
    " > and your solution should support batched inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AffineFunction(nn.Module):\n",
    "    \"\"\" An example of a very simple invertible function. \"\"\"\n",
    "    \n",
    "    def __init__(self, shift: float = 0., log_scale: float = 0.):\n",
    "        super().__init__()\n",
    "        self.shift = shift\n",
    "        self.log_scale = log_scale\n",
    "        \n",
    "    @property\n",
    "    def scale(self):\n",
    "        from math import exp\n",
    "        return exp(self.log_scale)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.scale * x + self.shift\n",
    "    \n",
    "    def invert(self, y):\n",
    "        return (y - self.shift) / self.scale\n",
    "    \n",
    "    def log_det_jacobi(self, x):\n",
    "        log_diag_jacobi = torch.full_like(x, self.log_scale)\n",
    "        return torch.sum(log_diag_jacobi, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a21410ea3111f78198d779de78e81eb9",
     "grade": false,
     "grade_id": "cell-6a11370f59c827dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jacobian(func: nn.Module, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian of a function at a given input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : nn.Module (D -> K)\n",
    "        The function to compute the Jacobian for.\n",
    "    x : (N, D) torch.Tensor\n",
    "        Batch of inputs to compute Jacobian for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Jacobian: (N, K, D)\n",
    "        The computed Jacobian determinants.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d173c0d4b1bd5c18591335f9e281087",
     "grade": true,
     "grade_id": "cell-3dd63e98f9692394",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "func = AffineFunction(2, 1)\n",
    "x_scalar = torch.linspace(-5, 5, 128).view(-1, 1)\n",
    "j_scalar = jacobian(func, x_scalar)\n",
    "assert j_scalar.shape == (128, 1, 1), (\n",
    "    \"ex1: jacobian output has wrong shape for scalar affine function (-1 point)\"\n",
    ")\n",
    "assert torch.allclose(func.log_det_jacobi(x_scalar).exp(), j_scalar.squeeze()), (\n",
    "    \"ex1: jacobian determinant does not match log determinant of scalar affine function (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05fbe52df291ceb7edf3889b088b10aa",
     "grade": true,
     "grade_id": "cell-6042890dbcd9659a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x_vector = torch.linspace(-5, 5, 128).view(32, 4)\n",
    "j_vector = jacobian(func, x_vector)\n",
    "assert j_vector.shape == (32, 4, 4), (\n",
    "    \"ex1: jacobian output has wrong shape for vector affine function (-1 point)\"\n",
    ")\n",
    "assert torch.allclose(func.log_det_jacobi(x_vector).exp(), torch.det(j_vector)), (\n",
    "    \"ex1: jacobian determinant does not match log determinant of vector affine function (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3162b194b78f87ea29bfec2966d58c38",
     "grade": true,
     "grade_id": "cell-390cb30ce051aefa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "func = nn.Sequential(nn.ELU(), nn.Linear(4, 3))\n",
    "x = torch.linspace(-5, 5, 128).view(32, 4)\n",
    "j = jacobian(func, x)\n",
    "assert j.shape == (32, 3, 4), (\n",
    "    \"ex1: jacobian output has wrong shape for fc layer (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 2: Change of Variables (2 points)\n",
    "\n",
    "With an invertible module like `AffineFunction`, \n",
    "implementing the change of variables formula should not be too hard.\n",
    "\n",
    " > Implement the `cov_pdf` function to compute the probability density\n",
    " > of the image of a function using the change of variables formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43921551b524f9924fe160900d053ee5",
     "grade": false,
     "grade_id": "cell-98d0f1ee74179d96",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def cov_pdf(y: torch.Tensor, func: nn.Module,\n",
    "            x_dist: distributions.Distribution):\n",
    "    \"\"\"\n",
    "    Compute the probability density using the change of variables formula.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : (N, D) torch.Tensor\n",
    "        Batch of values to compute probability density for.\n",
    "    func : nn.Module\n",
    "        The invertible function in the change of variables formula.\n",
    "    x_dist : Distribution\n",
    "        The distribution of the inputs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p_y : (N, ) torch.Tensor\n",
    "        Probability density for each value in the batch.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c8f0a6f5fffaf26538114c065c17fbb",
     "grade": true,
     "grade_id": "cell-815dbe698a8e505a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "func = AffineFunction(2, 1)\n",
    "z_gauss_1d = distributions.Normal(0., 1.)\n",
    "ref_gauss_1d = distributions.Normal(func.shift, func.scale)\n",
    "\n",
    "x_scalar = torch.linspace(-1., 1., 128).view(128, 1)\n",
    "p_cov = cov_pdf(x_scalar, func, z_gauss_1d)\n",
    "assert torch.allclose(p_cov, ref_gauss_1d.log_prob(x_scalar).exp()[:, 0]), (\n",
    "    \"ex2: cov_pdf outputs incorrect density for 1d affine function (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1b8e74515a6e36b2539516060574040",
     "grade": true,
     "grade_id": "cell-54e8d5064fad93aa",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f03bc2c776247ec35f9ef021b687f2e",
     "grade": true,
     "grade_id": "cell-544804d07b354ef7",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b4a02f006557cb519584931431b7cb5",
     "grade": true,
     "grade_id": "cell-e83430f16162ca87",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "func = AffineFunction(2, 1)\n",
    "z_mean, z_std = torch.zeros(4), torch.ones(4)\n",
    "z_gauss_4d = distributions.Normal(z_mean, z_std)\n",
    "ref_mean, ref_cov = z_mean + func.shift, func.scale ** 2 * torch.eye(4)\n",
    "ref_gauss_4d = distributions.MultivariateNormal(ref_mean, ref_cov)\n",
    "\n",
    "x_vector = torch.linspace(-1., 1., 128).view(32, 4)\n",
    "p_cov = cov_pdf(x_vector, func, z_gauss_4d)\n",
    "assert torch.allclose(p_cov, ref_gauss_4d.log_prob(x_vector).exp()), (\n",
    "    \"ex2: cov_pdf outputs incorrect density for 4d affine function (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Normalising Flows\n",
    "\n",
    "Given some input distribution and an invertible function,\n",
    "the change of variables formula gives the distribution of the outputs.\n",
    "Equivalently, it can be used to get the distribution of the inputs,\n",
    "given the output distribution:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  p_X(\\mathbf{x}) = p_Y(g(\\mathbf{x})) \\cdot \\left|\\det \\mathcal{J}_g(\\mathbf{x})\\right|.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Instead of fixing the invertible function in the formula, \n",
    "it is also possible to fix the input and output distributions.\n",
    "With this mindset, the change of variables formula\n",
    "provides a way to learn a function that is able to\n",
    "transform one distribution to some other distribution.\n",
    "\n",
    "Normalising flows are parameterised invertible functions\n",
    "that can be used to learn a function that can transform a distribution.\n",
    "More concretely, they map a complex data distribution, $X$,\n",
    "to some very simple target distribution, $Y$.\n",
    "To make normalising flows a practical generative model, \n",
    "two key entities should be easy to compute:\n",
    " 1. the inverse: \n",
    "    To generate new data, we sample from the simple distribution \n",
    "    and use the inverse of the normalising flow to transform it \n",
    "    to a sample in the data distribution.\n",
    " 2. the (log-)determinant of the Jacobian:\n",
    "    Normalising flows are trained to maximise the likelihood (see below),\n",
    "    which can be computed using the change of variables formula.\n",
    "    The logarithm stems from the fact that the log-likelihood\n",
    "    has some practical advantages over the plain likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 3: Invertible Networks (5 points)\n",
    "\n",
    "Neural networks are in general not (easily) invertible,\n",
    "and therefore not suited for normalising flows.\n",
    "One solution is to make use of so-called *coupling layers*.\n",
    "A coupling layer simply splits its inputs in two parts\n",
    "and computes the two parts of the outputs separately.\n",
    "One part of the output is simply one of the input parts.\n",
    "This input part is also used as input to a *conditioner network*.\n",
    "The conditioner network computes the parameters for an invertible transformation\n",
    "that is used to compute the second part of the output from the other input part.\n",
    "\n",
    " > Implement the `AdditiveCoupling` module below\n",
    " > so that it implements an invertible coupling layer.\n",
    " > Use $\\tau(x\\mathbin{;}\\theta) = x + \\theta$ for the invertible transformation.\n",
    " > Apart from the forward pass, you will also need to implement\n",
    " > the inverse function and the logarithm of\n",
    " > the absolute determinant of the jacobian.\n",
    " \n",
    "**Hint:** You want to make sure that the identity-part of the coupling does not always operate on the same pixels (when stacking multiple coupling layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bea594eb8b9c87f1d5759870a379103",
     "grade": false,
     "grade_id": "cell-2cf1cb4f6c389fb6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdditiveCoupling(nn.Module):\n",
    "    \"\"\" Coupling layer with additive coupling law. \"\"\"\n",
    "    \n",
    "    def __init__(self, sub_net: nn.Module):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        sub_net : nn.Module\n",
    "            The sub network to use as conditioner.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conditioner = sub_net\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Compute forward pass + log determinant of jacobian. \"\"\"\n",
    "        y = self._forward(x)\n",
    "        ldj = self.log_det_jacobi(x)\n",
    "        return y, ldj\n",
    "    \n",
    "    def _forward(self, x):\n",
    "        \"\"\" Convert inputs to outputs. \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def invert(self, y):\n",
    "        \"\"\" Convert outputs to inputs. \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def log_det_jacobi(self, x):\n",
    "        \"\"\" Compute logarithm of Jacobian determinant. \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d37a660f736361fbff99bc791ca2514",
     "grade": true,
     "grade_id": "cell-04a9d578925ae1fb",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = torch.ones(7, 40)\n",
    "func = AdditiveCoupling(nn.Linear(20, 20))\n",
    "y = func._forward(x)\n",
    "assert y.shape == (7, 40), (\n",
    "    \"ex3: AdditiveCoupling._forward method produces outputs with incorrect shape (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "484861d9cb0c7fe174c8184ab58549ec",
     "grade": true,
     "grade_id": "cell-5420a7bdd34dbdc1",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f30090e4c32c086f53d8d61a888d44e0",
     "grade": true,
     "grade_id": "cell-6fa5332df473d4c3",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdafc89fd4600366b773696727aa7fb2",
     "grade": true,
     "grade_id": "cell-41740b225594dd00",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "round_trip = func.invert(y)\n",
    "assert round_trip.shape == (7, 40), (\n",
    "    \"ex3: AdditiveCoupling.invert method produces outputs with incorrect shape (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4dd15bb12885062db39f8721d76c37b",
     "grade": true,
     "grade_id": "cell-a86cf32f8f21b964",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert torch.allclose(x, round_trip), (\n",
    "    \"ex3: AdditiveCoupling.invert does not implement inverse operation (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e3cc42c7d13ca4faa792621d1b4e625",
     "grade": true,
     "grade_id": "cell-39fc4ce6867a8aa1",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0e1f89b68a1680f836206c4127ecd90",
     "grade": true,
     "grade_id": "cell-022de385dfc7da01",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "873d6de04df357dc2baf96b8c716930a",
     "grade": true,
     "grade_id": "cell-42dce0bfd889565d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = torch.ones(7, 40)\n",
    "ldj = func.log_det_jacobi(x)\n",
    "assert ldj.shape == (7, ), (\n",
    "    \"ex3: AdditiveCoupling.log_det_jacobi produces output with incorrect shape (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a00883236bbe7797171b52cc408f721",
     "grade": true,
     "grade_id": "cell-ca3cc0898c9afaf7",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 4: Likelihood Maximisation (3 points)\n",
    "\n",
    "Adding the parameters of the normalising flow \n",
    "in the change of variable formula, gives the following expression\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  p_X(\\mathbf{x} \\mathbin{;} \\mathbf{w}) = p_Y(g(\\mathbf{x} \\mathbin{;} \\mathbf{w})) \\cdot \\left|\\det \\mathcal{J}_g(\\mathbf{x} \\mathbin{;} \\mathbf{w})\\right|.\n",
    "\\end{aligned}$$\n",
    "\n",
    "This is exactly the probability of the data, \n",
    "given the parameters, or the *likelihood*, of the normalising flow.\n",
    "By maximising this likelihood, we obtain the function\n",
    "that is most likely to be the true transformation to go from $X$ to $Y$.\n",
    "Note that maximising the likelihood is equivalent to \n",
    "minimising the negative logarithm of the likelihood.\n",
    "This gives us the objective for training a normalising flow.\n",
    "\n",
    " > Complete the implementation of the `NegativeLogLikelihood` module\n",
    " > to compute the *negative log-likelihood* for a normalising flow.\n",
    " > Use the `reduction` method to combine the results \n",
    " > from multiple samples in a batch.\n",
    " > Also, distinguish between sample density and pixel densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5a41f5cd5c9e0fd55682761473a5abb",
     "grade": false,
     "grade_id": "cell-6562fb16638c07ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NegativeLogLikelihood(nn.Module):\n",
    "    \"\"\" Negative Log-Likelihood of normalising flow. \"\"\"\n",
    "    \n",
    "    def __init__(self, prior: distributions.Distribution,\n",
    "                 reduction: str = None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        prior : Distribution\n",
    "            The output distribution of the normalising flow.\n",
    "        reduction : str, optional\n",
    "            How to aggregate results for multiple samples in a batch.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.prior = prior\n",
    "        \n",
    "        if reduction is None or reduction == 'mean':\n",
    "            self.reduction = torch.mean\n",
    "        elif reduction == 'sum':\n",
    "            self.reduction = torch.sum\n",
    "        elif reduction == 'none':\n",
    "            self.reduction = lambda x, **kwargs: x\n",
    "        else:\n",
    "            raise ValueError(f\"unknown reduction: '{reduction}'\")\n",
    "        \n",
    "    def forward(self, outputs):\n",
    "        \"\"\"\n",
    "        Compute negative log-likelihood on batch of NF outputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : ((N, D) torch.Tensor, (N, ) torch.Tensor)\n",
    "            Batch of outputs from the normalising flow.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nll : torch.Tensor\n",
    "            The aggregated negative log-likelihood for this batch.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76234d6cb1edfd50ba84653c4aae5178",
     "grade": true,
     "grade_id": "cell-efb16f1bcd52c794",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete\n",
    "y = torch.randn(3, 10)\n",
    "nll = NegativeLogLikelihood(distributions.Normal(0., 1.), reduction=\"none\")\n",
    "loss = nll((y, torch.ones(len(y))))\n",
    "assert loss.shape == (3, ), (\n",
    "    \"ex4: NegativeLogLikelihood produces outputs with wrong shape for none reduction (-1 point)\"\n",
    ")\n",
    "assert torch.all(loss > 0), (\n",
    "    \"ex4: NegativeLogLikelihood produces non-positive outputs for steep function (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ce534207882a3bc982431812fe2d3a7",
     "grade": true,
     "grade_id": "cell-5f61fbf778a4e4ab",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete\n",
    "nll = NegativeLogLikelihood(distributions.Normal(0., 1.), reduction=\"mean\")\n",
    "loss = nll((y, torch.ones(len(y))))\n",
    "assert loss.shape == (), (\n",
    "    \"ex4: NegativeLogLikelihood produces outputs with wrong shape for mean reduction (-0.5 points)\"\n",
    ")\n",
    "nll = NegativeLogLikelihood(distributions.Normal(0., 1.), reduction=\"sum\")\n",
    "loss = nll(func(x))\n",
    "assert loss.shape == (), (\n",
    "    \"ex4: NegativeLogLikelihood produces outputs with wrong shape for sum reduction (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af8c73ea3e3f66e994d8fb77c12c6cd6",
     "grade": true,
     "grade_id": "cell-529cd1fa030aa420",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete\n",
    "nll1 = NegativeLogLikelihood(distributions.Normal(0., 1.))\n",
    "nll2 = NegativeLogLikelihood(distributions.Normal(0., 2.))\n",
    "assert torch.any(nll1((y, torch.ones(len(y)))) != nll2((y, torch.ones(len(y))))), (\n",
    "    \"ex4: NegativeLogLikelihood ignores prior (-0.5 points)\"\n",
    ")\n",
    "assert torch.any(nll1((y, torch.ones(len(y)))) != nll1((y, torch.zeros(len(y))))), (\n",
    "    \"ex4: NegativeLogLikelihood ignores log_det (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4e239725dd903408a17c75a4f36371b",
     "grade": true,
     "grade_id": "cell-f7442c301f442343",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 5: Training a Normalising Flow (2 points)\n",
    "\n",
    "With the invertible network layers and objective function,\n",
    "we have everything we need to train a normalising flow.\n",
    "Since multi-layer networks are more powerful,\n",
    "you will find the `NormalisingFlow` class below useful.\n",
    "\n",
    " > Construct a normalising flow with multiple coupling layers.\n",
    " > Each coupling layer should have a multi-layer conditioner.\n",
    " > The training results should produce reasonably looking images.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NormalisingFlow(nn.Sequential):\n",
    "    \"\"\" Sequence of invertible layers, forming a normalising flow. \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Compute forward pass + log determinant of jacobian. \"\"\"\n",
    "        ld = 0\n",
    "        for m in self:\n",
    "            ld = ld + m.log_det_jacobi(x)\n",
    "            x = m._forward(x)\n",
    "        return x, ld\n",
    "    \n",
    "    def _forward(self, x):\n",
    "        \"\"\" Convert inputs to outputs. \"\"\"\n",
    "        for m in self:\n",
    "            x = m._forward(x)\n",
    "        return x\n",
    "    \n",
    "    def invert(self, y):\n",
    "        \"\"\" Convert outputs to inputs. \"\"\"\n",
    "        for m in reversed(self):\n",
    "            y = m.invert(y)\n",
    "        return y\n",
    "    \n",
    "    def log_det_jacobi(self, x):\n",
    "        \"\"\" Compute logarithm of Jacobian determinant. \"\"\"\n",
    "        ld = 0\n",
    "        for m in self:\n",
    "            ld = ld + m.log_det_jacobi(x)\n",
    "        return ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "516ca7ed903ddf3efa2da62ab6d2fb72",
     "grade": false,
     "grade_id": "cell-4db693123b3f560a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_multi_flow(num_in: int = 784):\n",
    "    \"\"\"\n",
    "    Create a normalizing flow with multiple multi-layer coupling layers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_in : int\n",
    "        Number of input neurons.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0b942aea8a6600d9dd3eafffae2302c",
     "grade": true,
     "grade_id": "cell-43a8db84156bfb9f",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "flow = build_multi_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56b26ac6707a2cb1656fac2ac8193c40",
     "grade": true,
     "grade_id": "cell-c06adb2fe1eaf778",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39416f9ff309fc64f233f1953003f55f",
     "grade": true,
     "grade_id": "cell-39f58ba2eb83f077",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = torch.randn(3, 784)\n",
    "y = flow._forward(x)\n",
    "assert y.shape == x.shape, (\n",
    "    \"ex5: build_multi_flow does not produce outputs with correct shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "pre_process = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((.1307, ), (.3081, )),\n",
    "    nn.Flatten(0)\n",
    "])\n",
    "mnist = datasets.MNIST(data_root, transform=pre_process)\n",
    "loader = DataLoader(mnist, batch_size=128, shuffle=True)\n",
    "\n",
    "# objective\n",
    "mean = torch.zeros(784).to(device)\n",
    "std = torch.ones(784).to(device)\n",
    "prior = distributions.Normal(mean, std)\n",
    "nll = NegativeLogLikelihood(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "flow = build_multi_flow().to(device)\n",
    "trainer = NormalisingFlowTrainer(\n",
    "    model=flow,\n",
    "    criterion=nll,\n",
    "    optimiser=optim.Adam(flow.parameters(), lr=1e-3)\n",
    ")\n",
    "\n",
    "trainer.train(loader, num_epochs=9, vis_every=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
