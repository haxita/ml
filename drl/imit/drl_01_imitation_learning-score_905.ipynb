{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRnY963jq1CW"
      },
      "source": [
        "Authors: Hofmarcher\n",
        "\n",
        "Date: 20-03-2023\n",
        "\n",
        "---\n",
        "\n",
        "This file is part of the \"Deep Reinforcement Learning\" lecture material. The following copyright statement applies to all code within this file.\n",
        "\n",
        "Copyright statement:\n",
        "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eryXuKadrmRK"
      },
      "source": [
        "## Enable GPU Acceleration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e5BB3G1hXL5"
      },
      "source": [
        "---\n",
        "Before you start exploring this notebook make sure that GPU support is enabled.\n",
        "To enable the GPU backend for your notebook, go to **Edit** → **Notebook Settings** and set **Hardware accelerator** to **GPU**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBUs4yMsgRSz"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D64rNsQCyL6Q"
      },
      "source": [
        "Install Gymnasium and dependencies to render the environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqL6W_Gkgp9a"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y swig python3-numpy python3-dev cmake zlib1g-dev libjpeg-dev xvfb ffmpeg xorg-dev python3-opengl libboost-all-dev libsdl2-dev\n",
        "!pip install gymnasium==0.29.0 gymnasium[box2d] pyvirtualdisplay imageio-ffmpeg moviepy==1.0.3\n",
        "!pip install onnx onnx2pytorch==0.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ECmcPAOnhR4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Auxiliary Python imports\n",
        "import os\n",
        "import math\n",
        "import io\n",
        "import base64\n",
        "import random\n",
        "import shutil\n",
        "from time import time, strftime\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "import onnx\n",
        "from onnx2pytorch import ConvertModel\n",
        "\n",
        "# Environment import and set logger level to display error only\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box\n",
        "from gymnasium import logger as gymlogger\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "gymlogger.set_level(gym.logger.ERROR)\n",
        "\n",
        "# Plotting and notebook imports\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML, clear_output\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCKe0oEsWDMe"
      },
      "source": [
        "# Select device for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf0U5yqXWDMe"
      },
      "source": [
        "By default we train on GPU if one is available, otherwise we fall back to the CPU.\n",
        "If you want to always use the CPU change accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1npX57pWDMe"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \" + str(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De3CuyBLgXos"
      },
      "source": [
        "# Setup Google Drive mount to store your results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vpd2i1MumjP0"
      },
      "outputs": [],
      "source": [
        "use_google_drive = True\n",
        "if use_google_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IuPNzGVgGR0"
      },
      "source": [
        "# Download Dataset and Expert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy79qvPFfLRp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Download training and validation datasets\n",
        "!wget --no-check-certificate 'https://cloud.ml.jku.at/s/citYJKPgmAGrHGy/download' -O expert.onnx\n",
        "!wget --no-check-certificate 'https://cloud.ml.jku.at/s/yJ2ZsfqTos3Jn9y/download' -O train.zip\n",
        "!wget --no-check-certificate 'https://cloud.ml.jku.at/s/3DxHLiqxTddepp8/download' -O val.zip\n",
        "\n",
        "# Unzip datasets\n",
        "!unzip -q -o train.zip\n",
        "!unzip -q -o val.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def convert_to_4stack(in_dir=\"train\", out_dir=\"train_4stack\"):\n",
        "    # 1) 读取所有 npz 文件\n",
        "    files = sorted(glob(os.path.join(in_dir, \"*.npz\")))\n",
        "    if not files:\n",
        "        print(f\"No npz files found in {in_dir}!\")\n",
        "        return\n",
        "\n",
        "    states = []\n",
        "    actions = []\n",
        "    for f in files:\n",
        "        data = np.load(f)\n",
        "        # 这里假设 data[\"state\"] 形状是 (84,84)\n",
        "        # 以及 data[\"action\"] 是一个 int\n",
        "        s = data[\"state\"]\n",
        "        a = data[\"action\"]\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "\n",
        "    # 2) 把单帧拼成 4帧 堆叠\n",
        "    new_states = []\n",
        "    new_actions = []\n",
        "\n",
        "    # 从第3索引开始，才能拿到 [i-3, i-2, i-1, i]\n",
        "    # 所以有效 i ∈ [3, len(states)-1]\n",
        "    for i in range(3, len(states)):\n",
        "        # 堆叠 states[i-3], states[i-2], states[i-1], states[i] 在 axis=0 上\n",
        "        stack_4 = np.stack(states[i-3:i+1], axis=0)  # shape (4,84,84)\n",
        "        new_states.append(stack_4)\n",
        "        new_actions.append(actions[i])  # 用最后一帧 i 的动作\n",
        "\n",
        "    # 3) 把新数据写到 out_dir\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    for i in range(len(new_states)):\n",
        "        # 给新文件起个类似 000000.npz 的名字\n",
        "        out_file = os.path.join(out_dir, f\"{i:06d}.npz\")\n",
        "        np.savez_compressed(out_file, state=new_states[i], action=new_actions[i])\n",
        "\n",
        "    print(f\"Converted {len(new_states)} samples and saved to {out_dir}/\")\n",
        "\n",
        "# 以 train 为例\n",
        "convert_to_4stack(in_dir=\"train\", out_dir=\"train_4stack\")\n",
        "# val 同理\n",
        "convert_to_4stack(in_dir=\"val\", out_dir=\"val_4stack\")"
      ],
      "metadata": {
        "id": "TYWSD6lehh4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_files = [f for f in os.listdir('train') if f.endswith('.npz')]\n",
        "print(\"Train NPZ files:\", train_files)\n",
        "\n",
        "# 随便拿一个 npz 文件试试看\n",
        "example_file = os.path.join('train', train_files[0])\n",
        "data = np.load(example_file)\n",
        "print(\"Keys in this NPZ:\", list(data.keys()))\n",
        "\n",
        "# 每个 key 通常对应一个 numpy 数组，查看形状:\n",
        "for k in data.keys():\n",
        "    print(k, data[k].shape)"
      ],
      "metadata": {
        "id": "TofkNGC-TojS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_files = [f for f in os.listdir('train_4stack') if f.endswith('.npz')]\n",
        "print(\"Train NPZ files:\", train_files)\n",
        "\n",
        "# 随便拿一个 npz 文件试试看\n",
        "example_file = os.path.join('train_4stack', train_files[0])\n",
        "data = np.load(example_file)\n",
        "print(\"Keys in this NPZ:\", list(data.keys()))\n",
        "\n",
        "# 每个 key 通常对应一个 numpy 数组，查看形状:\n",
        "for k in data.keys():\n",
        "    print(k, data[k].shape)"
      ],
      "metadata": {
        "id": "4vC7ZW6EhxoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "# 假设你的文件名是 'expert.onnx'\n",
        "model_path = \"expert.onnx\"\n",
        "\n",
        "# 加载模型\n",
        "model = onnx.load(model_path)\n",
        "\n",
        "# 检查模型是否格式正确 (可选)\n",
        "onnx.checker.check_model(model)\n",
        "\n",
        "# 可以打印模型的大致结构\n",
        "print(onnx.helper.printable_graph(model.graph))"
      ],
      "metadata": {
        "id": "V_L18gmRd3p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_shape(value_info):\n",
        "    \"\"\"解析 ONNX 的 tensor shape，并返回 Python 列表。\"\"\"\n",
        "    shape = []\n",
        "    tensor_type = value_info.type.tensor_type\n",
        "    for dim in tensor_type.shape.dim:\n",
        "        # 如果 dim_value 为 0 或 -1，可能表示动态维度\n",
        "        if dim.dim_value > 0:\n",
        "            shape.append(dim.dim_value)\n",
        "        else:\n",
        "            # 某些模型使用动态维度，可以用 None 或 \"?\" 标识\n",
        "            shape.append(\"?\")\n",
        "    return shape\n",
        "\n",
        "# 查看模型的输入形状\n",
        "print(\"== Model Inputs ==\")\n",
        "for i, input_tensor in enumerate(model.graph.input):\n",
        "    shape = get_tensor_shape(input_tensor)\n",
        "    print(f\"Input {i} name:\", input_tensor.name)\n",
        "    print(f\"Input {i} shape:\", shape)\n",
        "\n",
        "# 查看模型的输出形状\n",
        "print(\"\\n== Model Outputs ==\")\n",
        "for i, output_tensor in enumerate(model.graph.output):\n",
        "    shape = get_tensor_shape(output_tensor)\n",
        "    print(f\"Output {i} name:\", output_tensor.name)\n",
        "    print(f\"Output {i} shape:\", shape)"
      ],
      "metadata": {
        "id": "CmMGB-DveC6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_files = sorted(glob(\"train/*.npz\"))\n",
        "# 假设想看前3个文件\n",
        "for f in train_files[:3]:\n",
        "    data = np.load(f)\n",
        "    state = data[\"state\"]\n",
        "    print(f\"{f}: {state.shape}\")"
      ],
      "metadata": {
        "id": "X5FLN1_5UozB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "train_files = sorted(glob(\"train_4stack/*.npz\"))\n",
        "# 假设想看前3个文件\n",
        "for f in train_files[:3]:\n",
        "    data = np.load(f)\n",
        "    state = data[\"state\"]\n",
        "    print(f\"{f}: {state.shape}\")"
      ],
      "metadata": {
        "id": "K4zN_3dIiEJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5chkQUwj4pT"
      },
      "source": [
        "# Auxiliary Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBSdD-22WDMf"
      },
      "source": [
        "The following cell contains classes and functions to provide some functionality for logging, plotting and exporting your model in the format required by the submission server.\n",
        "You are free to use your own logging framework if you wish (such as tensorboard or Weights & Biases).\n",
        "The logger is a very simple implementation of a CSV-file based logger.\n",
        "Additionally it creates a folder for each run with subfolders for model files, logs and videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrjYL01ojsAD",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Logger():\n",
        "    def __init__(self, logdir, params=None):\n",
        "        self.basepath = os.path.join(logdir, strftime(\"%Y-%m-%dT%H-%M-%S\"))\n",
        "        os.makedirs(self.basepath, exist_ok=True)\n",
        "        os.makedirs(self.log_dir, exist_ok=True)\n",
        "        if params is not None and os.path.exists(params):\n",
        "            shutil.copyfile(params, os.path.join(self.basepath, \"params.pkl\"))\n",
        "        self.log_dict = {}\n",
        "        self.dump_idx = {}\n",
        "\n",
        "    @property\n",
        "    def param_file(self):\n",
        "        return os.path.join(self.basepath, \"params.pkl\")\n",
        "\n",
        "    @property\n",
        "    def onnx_file(self):\n",
        "        return os.path.join(self.basepath, \"model.onnx\")\n",
        "\n",
        "    @property\n",
        "    def video_dir(self):\n",
        "        return os.path.join(self.basepath, \"videos\")\n",
        "\n",
        "    @property\n",
        "    def log_dir(self):\n",
        "        return os.path.join(self.basepath, \"logs\")\n",
        "\n",
        "    def log(self, name, value):\n",
        "        if name not in self.log_dict:\n",
        "            self.log_dict[name] = []\n",
        "            self.dump_idx[name] = -1\n",
        "        self.log_dict[name].append((len(self.log_dict[name]), time(), value))\n",
        "\n",
        "    def get_values(self, name):\n",
        "        if name in self.log_dict:\n",
        "            return [x[2] for x in self.log_dict[name]]\n",
        "        return None\n",
        "\n",
        "    def dump(self):\n",
        "        for name, rows in self.log_dict.items():\n",
        "            with open(os.path.join(self.log_dir, name + \".log\"), \"a\") as f:\n",
        "                for i, row in enumerate(rows):\n",
        "                    if i > self.dump_idx[name]:\n",
        "                        f.write(\",\".join([str(x) for x in row]) + \"\\n\")\n",
        "                        self.dump_idx[name] = i\n",
        "\n",
        "\n",
        "def plot_metrics(logger):\n",
        "    train_loss  = logger.get_values(\"training_loss\")\n",
        "    train_entropy  = logger.get_values(\"training_entropy\")\n",
        "    val_loss = logger.get_values(\"validation_loss\")\n",
        "    val_acc = logger.get_values(\"validation_accuracy\")\n",
        "\n",
        "    fig = plt.figure(figsize=(15,5))\n",
        "    ax1 = fig.add_subplot(131, label=\"train\")\n",
        "    ax2 = fig.add_subplot(131, label=\"val\",frame_on=False)\n",
        "    ax4 = fig.add_subplot(132, label=\"entropy\")\n",
        "    ax3 = fig.add_subplot(133, label=\"acc\")\n",
        "\n",
        "    ax1.plot(train_loss, color=\"C0\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.set_xlabel(\"Update (Training)\", color=\"C0\")\n",
        "    ax1.xaxis.grid(False)\n",
        "    ax1.set_ylim((0,4))\n",
        "\n",
        "    ax2.plot(val_loss, color=\"C1\")\n",
        "    ax2.xaxis.tick_top()\n",
        "    ax2.yaxis.tick_right()\n",
        "    ax2.set_xlabel('Epoch (Validation)', color=\"C1\")\n",
        "    ax2.xaxis.set_label_position('top')\n",
        "    ax2.xaxis.grid(False)\n",
        "    ax2.get_yaxis().set_visible(False)\n",
        "    ax2.set_ylim((0,4))\n",
        "\n",
        "    ax4.plot(train_entropy, color=\"C3\")\n",
        "    ax4.set_xlabel('Update (Training)', color=\"black\")\n",
        "    ax4.set_ylabel(\"Entropy\", color=\"C3\")\n",
        "    ax4.tick_params(axis='x', colors=\"black\")\n",
        "    ax4.tick_params(axis='y', colors=\"black\")\n",
        "    ax4.xaxis.grid(False)\n",
        "\n",
        "    ax3.plot(val_acc, color=\"C2\")\n",
        "    ax3.set_xlabel(\"Epoch (Validation)\", color=\"black\")\n",
        "    ax3.set_ylabel(\"Accuracy\", color=\"C2\")\n",
        "    ax3.tick_params(axis='x', colors=\"black\")\n",
        "    ax3.tick_params(axis='y', colors=\"black\")\n",
        "    ax3.xaxis.grid(False)\n",
        "    ax3.set_ylim((0,1))\n",
        "\n",
        "    fig.tight_layout(pad=2.0)\n",
        "    plt.show()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "\"\"\"\n",
        "def show_video(video_dir):\n",
        "    mp4list = glob(f'{video_dir}/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "def save_as_onnx(torch_model, sample_input, model_path):\n",
        "    torch.onnx.export(torch_model,             # model being run\n",
        "                    sample_input,              # model input (or a tuple for multiple inputs)\n",
        "                    f=model_path,              # where to save the model (can be a file or file-like object)\n",
        "                    export_params=True,        # store the trained parameter weights inside the model file\n",
        "                    opset_version=17,          # the ONNX version to export the model to - see https://github.com/microsoft/onnxruntime/blob/master/docs/Versioning.md\n",
        "                    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3QTC6sgj2Eq"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4xmbKPRWDMg"
      },
      "source": [
        "\\Use this dataset class to load the provided demonstrations. Furthermore, this dataset has functionality to add new samples to the dataset which you will need for implementing the DAgger algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9ihClXhjvOT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class DemonstrationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.files = sorted(glob(f\"{data_dir}/*.npz\"))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = np.load(self.files[idx])\n",
        "        #state = data[\"state\"][np.newaxis, ...].astype(np.float32)\n",
        "        state = data[\"state\"].astype(np.float32)\n",
        "        action = data[\"action\"]\n",
        "        return state / 255.0, action.item()\n",
        "\n",
        "    def append(self, states, actions):\n",
        "        offset = len(self) + 1\n",
        "        for i in range(len(states)):\n",
        "            filename = f\"{self.data_dir}/{offset+i:06}.npz\"\n",
        "            np.savez_compressed(filename, state=states[i], action=actions[i].astype(np.int32))\n",
        "            self.files.append(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vViEbSRZj_4x"
      },
      "source": [
        "# Inspect data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dUd_3KWWDMh"
      },
      "source": [
        "It is always a good idea to take a look at the data when you start working with a new dataset. Feel free to investigate the dataset further on your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYljPrjekEEL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Action Statistics\n",
        "dataset = DemonstrationDataset(\"train_4stack\")\n",
        "print(\"Number of samples: {}\".format(len(dataset)));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1itQJFxwkHYT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Action mapping from gymnasium.farama.org\n",
        "action_mapping = {\n",
        "    0: \"do nothing\",\n",
        "    1: \"steer left\",\n",
        "    2: \"steer right\",\n",
        "    3: \"gas\",\n",
        "    4: \"brake\"\n",
        "}\n",
        "\n",
        "# Visualize random frames\n",
        "idx = np.random.randint(len(dataset))\n",
        "state, action = dataset[idx]\n",
        "# store a single frame as we need it later for exporting an ONNX model (it needs a sample of the input for the export)\n",
        "sample_state = torch.Tensor(state).unsqueeze(0).to(device)\n",
        "# Display the sample\n",
        "print(f\"Action: {action_mapping[action]}\")\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(state[0]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87zShnVElwv7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# release memory\n",
        "del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcUOi1VRmPTm"
      },
      "source": [
        "# Define Policy Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R4iC1PIWDMh"
      },
      "source": [
        "You need to design a neural network architecture that is capable of mapping a state to an action.\n",
        "The input is a single image with the following properties:\n",
        "- Resolution of 84x84 pixels\n",
        "- Grayscale (meaning a single channel as opposed to three channels of an RGB image)\n",
        "- The values of each pixel should be between 0 and 1\n",
        "\n",
        "The output of the network should be one unit per possible action, as our environment has 5 actions that results in 5 output units.\n",
        "Your network must implement the forward function in order to be compatible with the evaluation script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbVFFMoSmSpO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, n_units_out, dropout_p=0.2):\n",
        "        \"\"\"\n",
        "        n_units_out: 动作数 (5)\n",
        "        dropout_p: 全连接层的dropout概率，简单正则化\n",
        "        \"\"\"\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "\n",
        "        # 注意：输入是(4,84,84)，因为FrameStack=4帧叠加（灰度）\n",
        "        # 如果只是一帧，可将 in_channels 调成1。题目中要求和专家一样堆叠了4帧，则 in_channels=4。\n",
        "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # 理论上经过上述卷积后尺寸： (4,84,84) -> (32, 20,20) -> (64,9,9) -> (64,7,7)\n",
        "        # 最终扁平化是 64*7*7=3136，这里和expert.onnx匹配\n",
        "        self.fc1 = nn.Linear(64*7*7, 512)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc2 = nn.Linear(512, n_units_out)  # 输出5个动作的logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (B, 4, 84, 84)\n",
        "        return shape: (B, 5)\n",
        "        \"\"\"\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, n_units_out, dropout_p=0.2, num_groups=4):\n",
        "        \"\"\"\n",
        "        n_units_out: 动作数 (5)\n",
        "        dropout_p: 全连接层的dropout概率，简单正则化\n",
        "        num_groups: GroupNorm 的分组数；需要整除通道数\n",
        "        \"\"\"\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "\n",
        "        # 卷积部分：3 层卷积 + GroupNorm + ReLU\n",
        "        # 注意 out_channels 需要被 num_groups 整除\n",
        "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)\n",
        "        self.gn1   = nn.GroupNorm(num_groups=num_groups, num_channels=32)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.gn2   = nn.GroupNorm(num_groups=num_groups, num_channels=64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.gn3   = nn.GroupNorm(num_groups=num_groups, num_channels=64)\n",
        "\n",
        "        # 展平\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # 全连接部分：一层 Linear + LayerNorm + ReLU + Dropout，最后再输出 Linear\n",
        "        self.fc1  = nn.Linear(64 * 7 * 7, 512)\n",
        "        self.ln1  = nn.LayerNorm(512)\n",
        "        self.drop = nn.Dropout(dropout_p)\n",
        "        self.fc2  = nn.Linear(512, n_units_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, 4, 84, 84)\n",
        "        return shape: (batch_size, 5)\n",
        "        \"\"\"\n",
        "        # 卷积 + GroupNorm + ReLU\n",
        "        x = F.relu(self.gn1(self.conv1(x)))\n",
        "        x = F.relu(self.gn2(self.conv2(x)))\n",
        "        x = F.relu(self.gn3(self.conv3(x)))\n",
        "\n",
        "        x = self.flatten(x)          # shape => (batch_size, 64*7*7 = 3136)\n",
        "        x = self.fc1(x)              # => (batch_size, 512)\n",
        "        x = self.ln1(x)              # LayerNorm\n",
        "        x = F.relu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)              # => (batch_size, 5)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VtJDaYK56bZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv8P8s4rnOZM"
      },
      "source": [
        "# Train behavioral cloning policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcfU-i6_WDMh"
      },
      "source": [
        "Now that you have a Dataset and a network you need to train your network.\n",
        "With behavioral cloning we want to imitate the behavior of the agent that produced the demonstration dataset as close as possible.\n",
        "This is basically supervised learning, where you want to minimize the loss of your network on the training and validation sets.\n",
        "\n",
        "Some tips as to what you need to implement:\n",
        "- choose the appropriate loss function (think on which kind of problem you are solving)\n",
        "- choose an optimizer and its hyper-parameters\n",
        "- optional: use a learning-rate scheduler\n",
        "- don't forget to evaluate your network on the validation set\n",
        "- store your model and training progress often so you don't loose progress if your program crashes\n",
        "\n",
        "In case you use the provided Logger:\n",
        "- `logger.log(\"training_loss\", <loss-value>)` to log a particular value\n",
        "- `logger.dump()` to write the current logs to a log file (e.g. after every episode)\n",
        "- `logger.log_dir`, `logger.param_file`, `logger.onnx_file`, `logger.video_dir` point to files or directories you can use to save files\n",
        "- you might want to specify your google drive folder as a logdir in order to automatically sync your results\n",
        "- if you log the metrics specified in the `plot_metrics` function you can use it to visualize your training progress (or take it as a template to plot your own metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM7ylckLnRnP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# choose the batchsize for training\n",
        "batch_size = 64\n",
        "\n",
        "# Datasets\n",
        "train_set = DemonstrationDataset(\"train_4stack\")\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, num_workers=2, shuffle=True, drop_last=False, pin_memory=True)\n",
        "val_set = DemonstrationDataset(\"val_4stack\")\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, num_workers=2, shuffle=False, drop_last=False, pin_memory=True)\n",
        "\n",
        "# Specify the google drive mount here if you want to store logs and weights there (and set it up earlier)\n",
        "# You can also choose to use a different logging framework such as tensorboard (not recommended on Colab) or Weights & Biases (highly recommended)\n",
        "logger = Logger(\"logdir\")\n",
        "print(\"Saving state to {}\".format(logger.basepath))\n",
        "\n",
        "# Network\n",
        "model = PolicyNetwork(n_units_out=5)\n",
        "model = model.to(device)\n",
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Trainable Parameters: {}\".format(num_trainable_params))\n",
        "\n",
        "######################\n",
        "### YOUR CODE HERE ###\n",
        "######################\n",
        "# Implement your training and evaluation loop\n",
        "# feel free to define your own functions for training and evaluation\n",
        "\n",
        "# If you want to export your model as an ONNX file use the following code as template\n",
        "# If you use the provided logger you can use this directly\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, logger=None):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_entropy = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for states, actions in dataloader:\n",
        "        # states shape: (B, 1, 84, 84)， 但在环境中是4帧stack，因此这里需要扩维\n",
        "        #  如果已经在 DemonstrationDataset 做了 (1,84,84)，则仍需注意输入通道维度\n",
        "        #  这里假设示例数据中实际只有单帧，那么可以简单 replicate 到4通道\n",
        "        #  或者你在收集数据时就是4帧堆叠，这要看你数据本身如何准备。\n",
        "        # 如果演示数据只有单帧，可用 repeat 在通道维度扩成4。\n",
        "        # 如果你已经保证数据本身就是(4,84,84)，就不需要这一步了。\n",
        "        states = states.to(device)  # shape (B, 1, 84, 84)\n",
        "        #states = states.repeat(1,4,1,1)  # (B,4,84,84)——仅当原始数据只有1帧时\n",
        "\n",
        "        actions = actions.to(device)\n",
        "\n",
        "        logits = model(states)\n",
        "        loss = criterion(logits, actions)\n",
        "\n",
        "        # 计算分类分布的熵（仅作monitor）\n",
        "        dist = Categorical(logits=logits)\n",
        "        entropy = dist.entropy().mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # 累计\n",
        "        bs = states.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_entropy += entropy.item() * bs\n",
        "        total_samples += bs\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_entropy = total_entropy / total_samples\n",
        "\n",
        "    if logger is not None:\n",
        "        logger.log(\"training_loss\", avg_loss)\n",
        "        logger.log(\"training_entropy\", avg_entropy)\n",
        "        logger.dump()\n",
        "\n",
        "    return avg_loss, avg_entropy\n",
        "\n",
        "\n",
        "def validate(model, dataloader, logger=None):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for states, actions in dataloader:\n",
        "            states = states.to(device)\n",
        "            # 同理，如需把单帧数据扩成4帧\n",
        "            #states = states.repeat(1,4,1,1)\n",
        "\n",
        "            actions = actions.to(device)\n",
        "\n",
        "            logits = model(states)\n",
        "            loss = criterion(logits, actions)\n",
        "\n",
        "            # 计算accuracy\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct = (preds == actions).sum().item()\n",
        "\n",
        "            bs = states.size(0)\n",
        "            total_loss += loss.item() * bs\n",
        "            total_correct += correct\n",
        "            total_samples += bs\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_acc = total_correct / total_samples\n",
        "\n",
        "    if logger is not None:\n",
        "        logger.log(\"validation_loss\", avg_loss)\n",
        "        logger.log(\"validation_accuracy\", avg_acc)\n",
        "        logger.dump()\n",
        "\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 训练多个epoch\n",
        "# ---------------------------\n",
        "n_epochs = 3\n",
        "for epoch in range(n_epochs):\n",
        "    print(f\"Epoch [{epoch+1}/{n_epochs}]\")\n",
        "    train_loss, train_entropy = train_one_epoch(model, train_loader, optimizer, logger=logger)\n",
        "    val_loss, val_acc = validate(model, val_loader, logger=logger)\n",
        "    print(f\"  Train Loss: {train_loss:.4f}, Entropy: {train_entropy:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "plot_metrics(logger)\n",
        "\n",
        "\n",
        "save_as_onnx(model, sample_state, logger.onnx_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PelO4qdwuwK"
      },
      "source": [
        "# Evaluate the agent in the real environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVrASW_Hy1lo"
      },
      "source": [
        "### Environment and Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTFAzdfiaYmu"
      },
      "source": [
        "We provide some wrappers you need in order to get the same states from the environment as in the demonstration dataset.\n",
        "Additionally the `RecordState` wrapper should be very helpful in collecting new samples for the DAgger algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJeEIolVw0OQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class CropObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.shape = shape\n",
        "        obs_shape = self.shape + env.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return observation[:self.shape[0], :self.shape[1]]\n",
        "\n",
        "\n",
        "class RecordState(gym.Wrapper):\n",
        "    def __init__(self, env: gym.Env, reset_clean: bool = True):\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "\n",
        "        assert env.render_mode is not None\n",
        "        self.frame_list = []\n",
        "        self.reset_clean = reset_clean\n",
        "\n",
        "    def step(self, action, **kwargs):\n",
        "        output = self.env.step(action, **kwargs)\n",
        "        self.frame_list.append(output[0])\n",
        "        return output\n",
        "\n",
        "    def reset(self, *args, **kwargs):\n",
        "        result = self.env.reset(*args, **kwargs)\n",
        "\n",
        "        if self.reset_clean:\n",
        "            self.frame_list = []\n",
        "        self.frame_list.append(result[0])\n",
        "\n",
        "        return result\n",
        "\n",
        "    def render(self):\n",
        "        frames = self.frame_list\n",
        "        self.frame_list = []\n",
        "        return frames\n",
        "\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def select_action(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.Tensor(state).unsqueeze(0).to(device) / 255.0 # rescale\n",
        "            logits = self.model(state)\n",
        "            if type(logits) is tuple:\n",
        "                logits = logits[0]\n",
        "            probs = Categorical(logits=logits)\n",
        "            return probs.sample().cpu().numpy()[0]\n",
        "\n",
        "\n",
        "def make_env(seed, capture_video=True):\n",
        "    env = gym.make(\"CarRacing-v2\", render_mode=\"rgb_array\", continuous=False)\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    if capture_video:\n",
        "        env = gym.wrappers.RecordVideo(env, logger.video_dir)\n",
        "\n",
        "    env = CropObservation(env, (84, 96))\n",
        "    env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "    env = gym.wrappers.GrayScaleObservation(env)\n",
        "    env = RecordState(env, reset_clean=True)\n",
        "    env = gym.wrappers.FrameStack(env, 4)\n",
        "    env.reset(seed=seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "    return env\n",
        "\n",
        "\n",
        "def run_episode(agent, show_progress=True, capture_video=True, seed=None):\n",
        "    env = make_env(seed=seed, capture_video=capture_video)\n",
        "    state, _ = env.reset()\n",
        "    score = 0\n",
        "    done = False\n",
        "    if show_progress:\n",
        "        progress = tqdm(desc=\"Score: 0\")\n",
        "\n",
        "    while not done:\n",
        "        #action = agent.select_action(state[-1][np.newaxis, ...])\n",
        "        action = agent.select_action(state)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        score += reward\n",
        "        done = terminated or truncated\n",
        "        if show_progress:\n",
        "            progress.update()\n",
        "            progress.set_description(\"Score: {:.2f}\".format(score))\n",
        "    env.close()\n",
        "\n",
        "    if show_progress:\n",
        "        progress.close()\n",
        "    if capture_video:\n",
        "        show_video(logger.video_dir)\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkjTFDi3y72N"
      },
      "source": [
        "## Evaluate behavioral cloning agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0JaMOvxzBio"
      },
      "source": [
        "Let's see how the agent is doing in the real environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrqzgG-bzXws",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_policy = Agent(model, device)\n",
        "score = run_episode(train_policy, show_progress=True, capture_video=True);\n",
        "print(f\"Score: {score:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtqNU8QXAaHP"
      },
      "source": [
        "Since we often have high variance when evaluating RL agents we should evaluate the agent multiple times to get a better feeling for its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY5BeIKXAZs8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_policy = Agent(model, device)\n",
        "n_eval_episodes = 10\n",
        "scores = []\n",
        "for i in tqdm(range(n_eval_episodes), desc=\"Episode\"):\n",
        "    scores.append(run_episode(train_policy, show_progress=False, capture_video=False))\n",
        "    print(\"Score: %d\" % scores[-1])\n",
        "print(\"Mean Score: %.2f (Std: %.2f)\" %(np.mean(scores), np.std(scores)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmDAORUGhJQD"
      },
      "source": [
        "# DAGGER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8Y8WBIkXvlP"
      },
      "source": [
        "Now we can implement DAgger, you have downloaded a relatively well trained model you can use as an expert for this purpose.\n",
        "\n",
        "Load expert model that is provided as ONNX file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqFaY_8ZprUw"
      },
      "source": [
        "## Load the expert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QxdNRHnpw4V",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Load expert\n",
        "expert_model = ConvertModel(onnx.load(\"expert.onnx\"))\n",
        "expert_model = expert_model.to(device)\n",
        "# Freeze expert weights\n",
        "for p in expert_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "expert_policy = Agent(expert_model, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check expert performance\n",
        "\n",
        "def evaluate_expert(policy, env, n_episodes=5, render=False):\n",
        "    \"\"\"\n",
        "    使用给定的策略 (policy) 在 env 中跑 n_episodes 条回合，\n",
        "    并统计平均得分。\n",
        "\n",
        "    policy: 你的 expert_policy (Agent) 对象\n",
        "    env: Gym 环境\n",
        "    n_episodes: 评估多少回合\n",
        "    render: 是否在每步调用 env.render() 做实时可视化 (本地环境可用)\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        obs, _ = env.reset()  # 或 obs = env.reset() 取决于 Gym 版本\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "\n",
        "        while not done:\n",
        "            # 从 policy 获取动作\n",
        "            # 注意 CarRacing-v2 返回的 obs 可能是 LazyFrames 或 shape (4,84,84) 之类\n",
        "            # 视你的封装而定:\n",
        "            action = policy.get_action(obs)\n",
        "\n",
        "            # 交互一步\n",
        "            obs, reward, done, info, _ = env.step(action)  # 如果你的 Gym 版本无 _，可删除\n",
        "            episode_reward += reward\n",
        "\n",
        "            # 可选，实时渲染\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "        scores.append(episode_reward)\n",
        "        print(f\"Episode {ep} Score: {episode_reward:.2f}\")\n",
        "\n",
        "    mean_score = np.mean(scores)\n",
        "    print(f\"\\nExpert average score over {n_episodes} episodes: {mean_score:.2f}\")\n",
        "    return mean_score\n",
        "\n",
        "\n",
        "mean_score = evaluate_expert(expert_policy, env, n_episodes=5, render=False)\n",
        "print(\"Final mean score:\", mean_score)"
      ],
      "metadata": {
        "id": "x1DG0sxLjLP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmYXqfOZX0Yr"
      },
      "source": [
        "Next, you have to implement the DAgger algorithm (see slides for details). This function implements the core idea of DAgger:\n",
        "\n",
        "\n",
        "1. Choose the policy with probability beta\n",
        "2. Sample T-step trajectories using this policy\n",
        "3. Label the gathered states with the expert\n",
        "\n",
        "The aggregation and training part are already implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jso18L4z2Gio",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# inner loop of DAgger\n",
        "def dagger(env, train_policy, expert_policy, dataset, beta=1.,max_steps=4000):\n",
        "\n",
        "    ######################\n",
        "    ### YOUR CODE HERE ###\n",
        "    ######################\n",
        "\n",
        "    # Implement DAgger algorithm here\n",
        "    # 1) Choose a policy (sample according to beta)\n",
        "    # 2) Sample T-step trajectory with the chosen policy\n",
        "    #    (T can be an entire episode or a single state, think about what makes more sense here and implement it accordingly)\n",
        "    # 3) Label the state (or states) with your expert if they come from your training policy\n",
        "\n",
        "    #### Note ####\n",
        "    # To get an action for the current state from your training policy or expert policy:\n",
        "    # action = policy.select_action(state)\n",
        "    #\n",
        "    # Your training policy requires a single grayscale state while\n",
        "    # the expert policy requires four stacked grayscale states\n",
        "    # You can prepare your state for the policy like so:\n",
        "    # Train policy:\n",
        "    #      np.expand_dims(state[-1], 0)\n",
        "    # Expert policy:\n",
        "    #      state\n",
        "\n",
        "\n",
        "    # Due to the RecordState wrapper you can get the states from the environment by calling\n",
        "    # env.render()\n",
        "    # Doing so will clear the list and the next time you call .render() will return the new states since the last call.\n",
        "    # Note: be careful with the last state\n",
        "\n",
        "    # Finally, add collected states and the actions the expert would execute in them to the dataset\n",
        "    # dataset.append(states, actions)\n",
        "\n",
        "    \"\"\"\n",
        "    在env上跑一段episode，使用混合策略 (beta概率选专家动作，(1-beta)概率选训练策略)，\n",
        "    同时用专家策略对访问到的state打标签并追加到dataset中。\n",
        "    \"\"\"\n",
        "    states_buffer = []\n",
        "    actions_buffer = []\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    while (not done) and (steps < max_steps):\n",
        "        steps += 1\n",
        "\n",
        "        # 1) 混合策略：随机从专家和训练好的策略中采样动作\n",
        "        if np.random.rand() < beta:\n",
        "            action_exec = expert_policy.select_action(obs)\n",
        "        else:\n",
        "            action_exec = train_policy.select_action(obs)\n",
        "\n",
        "        # 与环境交互\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action_exec)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # 2) 始终用专家来给当前obs(或 next_obs)打标签\n",
        "        #    这里假设我们要为 “我们所处的这个时刻” 存储(state, expert_action)。\n",
        "        #    你也可以改为对 next_obs 做标注。\n",
        "        expert_action = expert_policy.select_action(obs)\n",
        "\n",
        "        # 需要转成 numpy 格式，以便 dataset.append() 保存\n",
        "        if torch.is_tensor(obs):\n",
        "            obs_to_store = obs.cpu().numpy()\n",
        "        else:\n",
        "            obs_to_store = obs\n",
        "\n",
        "        # 记录\n",
        "        states_buffer.append(obs_to_store)  # shape(4,84,84)\n",
        "        actions_buffer.append(np.array(expert_action, dtype=np.int32))\n",
        "\n",
        "        obs = next_obs\n",
        "\n",
        "    # 将收集到的数据添加到 dataset\n",
        "    dataset.append(states_buffer, actions_buffer)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svRD7bwpWDMj"
      },
      "source": [
        "Put everything together now.\n",
        "1. Create new samples using the DAgger algorithm\n",
        "2. Continue training your agent\n",
        "3. Export your fully trained agent as an ONNX file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXlZWzRsZ2dJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Specify the google drive mount here if you want to store logs and weights there (and set it up earlier)\n",
        "logger = Logger(\"logdir_dagger\")\n",
        "print(\"Saving state to {}\".format(logger.basepath))\n",
        "\n",
        "# start environment\n",
        "env = make_env(seed=42, capture_video=False)\n",
        "\n",
        "# Training\n",
        "######################\n",
        "### YOUR CODE HERE ###\n",
        "######################\n",
        "import torch.optim as optim\n",
        "optimizer_dagger = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "n_bc_epochs = 2\n",
        "for epoch in range(n_bc_epochs):\n",
        "    train_loss, train_ent = train_one_epoch(model, train_loader, optimizer_dagger)\n",
        "    val_loss, val_acc = validate(model, val_loader)\n",
        "    print(f\"[BC Epoch {epoch+1}/{n_bc_epochs}] train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "# 迭代若干次 DAgger\n",
        "n_dagger_iters = 10       # 例如迭代3轮\n",
        "n_dagger_train_epochs = 2  # 每次收集数据后再训练多少epoch\n",
        "beta = 1.0                # 初始专家概率\n",
        "\n",
        "for i in range(n_dagger_iters):\n",
        "    print(f\"\\n=== DAgger Iteration {i+1}/{n_dagger_iters} (beta={beta:.2f}) ===\")\n",
        "\n",
        "    # 1) 采样一条(或多条)轨迹，用expert打标签并存进 dataset\n",
        "    dagger(env, train_policy, expert_policy, train_set, beta=beta)\n",
        "\n",
        "    # 2) 用新的训练集训练：此时 train_set 增加了新的 (state,action)\n",
        "    #    重新构造 DataLoader 让其包含最新的数据\n",
        "    dagger_train_loader = torch.utils.data.DataLoader(\n",
        "        train_set, batch_size=64, shuffle=True, drop_last=False, pin_memory=True\n",
        "    )\n",
        "    for e in range(n_dagger_train_epochs):\n",
        "        tr_loss, tr_ent = train_one_epoch(model, dagger_train_loader, optimizer_dagger)\n",
        "        val_loss, val_acc = validate(model, val_loader)\n",
        "        print(f\"   [Epoch {e+1}/{n_dagger_train_epochs}] train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "    # 3) 逐步减少 beta，让代理更多地用自己的动作执行 (可选)\n",
        "    #    常见做法是 beta = beta * decay_rate 或 beta = beta - 1/n_iters\n",
        "    beta *= 0.9  # 这里只是示例写法\n",
        "\n",
        "    # 4) 记得更新 train_policy(内部的 model 已更新梯度)\n",
        "    train_policy = Agent(model, device)\n",
        "\n",
        "\n",
        "save_as_onnx(model, sample_state, logger.onnx_file)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iOeC98o9Pw-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "n_eval_episodes = 10\n",
        "scores = []\n",
        "for i in tqdm(range(n_eval_episodes), desc=\"Episode\"):\n",
        "    scores.append(run_episode(train_policy, show_progress=False, capture_video=False))\n",
        "    print(\"Score: %d\" % scores[-1])\n",
        "print(\"Mean Score: %.2f (Std: %.2f)\" %(np.mean(scores), np.std(scores)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}