{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a75d35-b20d-45ed-bbb2-2ac5d6520c7b",
   "metadata": {},
   "source": [
    "# Assignment 4: Variational Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86441c03-0a22-4dc5-9dba-835d5526930e",
   "metadata": {},
   "source": [
    "*Author:* Eric Volkmann / Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd5375-3bca-4d65-ac13-c926effae6f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373cb7f5-e9dc-4efc-a592-504c53f6b21d",
   "metadata": {},
   "source": [
    "In general latent variable models, we model the latent variables and the observations (data points) by a joint probability distribution $p(x, z)$. In the \"likelihood-based\" generative modelling setting, we aim to learn a model which maximizes the likelihood $p(x)$ of all data points. If the model is analytically tractable, we can explictly marginalize out the latent variables $z$:\n",
    "\n",
    "$ p(x) = \\int p(x, z) dz $\n",
    "\n",
    "We derived the maximum likelihood equation for the pPCA model in Assignment 2. For more complex, intractable models this approach fails as we don't have access to the ground truth latent encoder $p(x, z)$.\n",
    "\n",
    "In the last exercise sheet, we instead derived \"Evidence lower bound\" (ELBO), which is a lower bound on the evidence and that we can use as proxy objective to optimize our latent variable model, e.g. a simple VAE.\n",
    "\n",
    "$\\log p(x) \\geq \\mathbb{E} [ \\log \\frac{p(x,z)}{q_{\\phi}(z|x)} ]\n",
    "            = \\underbrace{\\mathbb{E}_{q_{\\phi}(z|x)} [ \\log p_{\\theta} (x | z) ]}_{\\text{reconstruction term}} - \\underbrace{D_{KL} ( q_{\\phi}(z | x) || p(z) )}_{\\text{prior matching term}}$\n",
    "\n",
    "In the variational autoencoder case, we learn an intermediate bottlenecking distribution $q_{\\phi}(z|x)$ that can be treated as an encoder; it transforms inputs into a distribution over possible latents. Simultaneously, we learn a deterministic function $p_{\\theta}(x|z)$ to convert a given latent vector $z$ into an observation $x$, which can be interpreted as a decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12d6802-0003-41e4-bca4-4f51a0f97552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/rBXVRvI.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://i.imgur.com/rBXVRvI.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eadd37-2e00-40ec-a807-a86adfcc54bc",
   "metadata": {},
   "source": [
    "A Hierarchical Variational Autoencoder (HVAE) is a generalization of a VAE that extends to multiple hierarchies over latent variables. Whereas in the general HVAE with $T$ hierarchical levels, each latent is allowed to condition on all previous latents, we focus on a special case which we call a Markovian HVAE (MHVAE). In a MHVAE,\n",
    "the generative process is a Markov chain; that is, each transition down the hierarchy is Markovian. The generative\n",
    "process is modeled as a Markov chain, where each latent $z_t$ is generated only from the previous latent $z_{t+1}$.\n",
    "Intuitively, and visually, this can be seen\n",
    "as simply stacking VAEs on top of each other. Mathematically, we represent the joint distribution and the posterior of a\n",
    "Markovian HVAE as:\n",
    "\n",
    "$p(x, z_{1:T}) = p(z_{T}) p_{\\theta}(x|z_1) \\prod_{t=2}^{T} p_{\\theta}(z_{t-1}|z_{t})$\n",
    "\n",
    "$q_{\\phi}(z_{1:T}|x) = q_{\\theta} (z_1 | x) \\prod_{t=2}^{T} q_{\\theta} (z_{t}|z_{t-1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbefe96-74a3-4ca8-ad7f-aa95c4b4acec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/teD56gK.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://i.imgur.com/teD56gK.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d31da8-7a2d-4b70-9f5a-cec98b811c52",
   "metadata": {},
   "source": [
    "The easiest way to think of a Variational Diffusion Model (VDM) is simply as a Markovian Hierarchical Variational Autoencoder with three key restrictions:\n",
    "\n",
    " i) The latent dimension is exactly equal to the data dimension\n",
    " \n",
    " ii) The structure of the latent encoder at each timestep is not learned; it is pre-defined as a linear Gaussian\n",
    "model. In other words, it is a Gaussian distribution centered around the output of the previous timestep\n",
    "\n",
    " iii) The Gaussian parameters of the latent encoders vary over time in such a way that the distribution of\n",
    "the latent at final timestep T is a standard Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0169c-459e-484c-a7f3-350b8e325768",
   "metadata": {},
   "source": [
    "From the second assumption, we know that the distribution of each latent variable in the encoder is a\n",
    "Gaussian centered around its previous hierarchical latent.\n",
    "The main takeaway is that $\\alpha_t$ is a (potentially learnable) coefficient that can\n",
    "vary with the hierarchical depth $t$, for flexibility. Mathematically, encoder transitions are denoted as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1}, (1-\\alpha_t) \\textbf{I})\n",
    "\\end{equation*}\n",
    "\n",
    "Note that our encoder distributions $q(x_t|x_{t−1})$ are no longer parameterized by $\\phi$, as they are completely\n",
    "modeled as Gaussians with defined mean and variance parameters at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8274565b-c25b-49a8-aa98-e2649bf70e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/VOSpXkR.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://i.imgur.com/VOSpXkR.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb40887-54c7-45e4-b0c2-49d8343efd49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise 1) The forward diffusion process (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75a049-032a-4016-b19b-71479a4c923f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Exercise 1a) Simulate the forward diffusion process (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d5abc-6235-453a-bb14-8dfd3515a57f",
   "metadata": {},
   "source": [
    "From the first restriction, with some abuse of notation, we can now represent both true data samples and latent variables as $x_t$, where $t = 0$ represents true data samples and $t \\in [1, T ]$ represents a corresponding latent with hierarchy indexed by $t$. \n",
    "\n",
    "The \"encoding process\" is referred to as the noise corruption or forward diffusion process in diffusion models\n",
    "\n",
    "and is defined as\n",
    "\\begin{align*}\n",
    "    q(x_t \\mid x_{t-1}) &= \\mathcal N(\\sqrt \\alpha_t x_{t-1}, (1-\\alpha_t) I ) && \\text{where} & \\alpha_t &\\in (0, 1) \\subset \\mathbb R. \n",
    "\\end{align*}\n",
    "If $\\alpha_t$ is close to 1, we draw $x_t$ from a normal distribution with mean close to $x_{t-1}$ and small variance. \n",
    "Simulate and visualize the forward diffusion process using a sample image from the CIFAR10 dataset for $T=500$. \n",
    "\n",
    "\n",
    "Use a fixed $\\alpha$ for all time steps. By visual inspection, tune $\\alpha$ to a value that lets $x_0$ smoothly blend into noise throughout the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4aba7bb-1b21-47ed-ba98-0d6ff41a26d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#import math\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as tfm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def imshow(img):\n",
    "    img = (img + 1) / 2 # rescale from [-1, 1] to [0, 1]\n",
    "    plt.imshow(np.transpose(img.numpy().clip(0, 1), (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=tfm.ToTensor())\n",
    "x = trainset[0][0] * 2 - 1 # scale to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cbc0b-0cf9-4d49-abd5-a98cf218005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269062e8-17c2-42bc-b5fb-0358f3ae4c0d",
   "metadata": {},
   "source": [
    "### Exercise 1b: Reparametrization trick for the forward process (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726b8068-2be9-4adc-99f1-b6b98dc4abc4",
   "metadata": {},
   "source": [
    "Again, the noise corruption or forward diffusion process, using a fixed $\\alpha$ across all timesteps, is defined as\n",
    "\\begin{align*}\n",
    "    q(x_t \\mid x_{t-1}) &= \\mathcal N(x_t; \\sqrt \\alpha x_{t-1}, (1-\\alpha) I ) && \\text{where} & \\alpha &\\in (0, 1) \\subset \\mathbb R. \n",
    "\\end{align*}\n",
    "Use the reparameterization trick to show that\n",
    "\\begin{align*}\n",
    "    q(x_t \\mid x_0) = \\mathcal N(x_t; \\sqrt{\\alpha^t} x_0, (1-\\alpha^t) I).\n",
    "\\end{align*}\n",
    "From this result, interpret the role of the parameter $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4abd79-3841-4a33-afc0-cd989388cab7",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9133f-9520-481f-8f3f-af345ba0e243",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise 2: A ELBO for VDMs (1.5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af30cabf-0f39-4937-bf84-421a3754cf3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/juJ4KMv.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://i.imgur.com/juJ4KMv.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc2fb5-9fe5-4b82-907b-50a956222d80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Note that our encoder distributions $q(x_{t}|x_{t−1})$ are no longer parameterized by $\\varphi$, as they are completely\n",
    "modeled as Gaussians with defined mean and variance parameters at each timestep. Therefore, in a VDM, we\n",
    "are only interested in learning conditionals $p_{\\theta} (x_{t-1} | x_{t})$, so that we can simulate new data. After optimizing\n",
    "the VDM, the sampling procedure is as simple as sampling Gaussian noise from $p(x_T)$ and iteratively running\n",
    "the denoising transitions $p_{\\theta} (x_{t-1}|x_t)$ for $T$ steps to generate a novel $x_0$.\n",
    "\n",
    "Like any HVAE, the VDM can be optimized by maximizing the ELBO. Derive the ELBO for the VDM and show that it written as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log p(x_0) \\geq \\underbrace{\\mathbb{E}_{q(x_1|x_0)}[\\log p_{\\theta}(x_0|x_1)]}_{\\text{reconstruction term}}\n",
    "- \\underbrace{\\mathbb{E}_{q(x_{T-1}|x_0)}[D_{KL}(q(x_{T}|x_{T-1} || p(x_{T}))]}_{\\text{prior matching term}}\n",
    "- \\sum_{t=1}^{T-1} \\underbrace{\\mathbb{E}_{q(x_{t-1}, x_{t+1}|x_0)} [D_{KL} (q(x_t|x_{t-1}) || p_{\\theta}(x_t|x_{t+1}))]}_{\\text{consistency term}}\n",
    "\\end{equation*}\n",
    "Interpret the differents terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf2493-fa79-4997-bd9e-1f20524b294c",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba2c5a-143c-48e9-847e-9c5e634a1f13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise 3: A smarter ELBO for VDMs (1.5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e7536a3-ed83-4bf4-bf6a-1df97c3886ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/Q08pjU3.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://i.imgur.com/Q08pjU3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7028c719-4e6e-423c-b802-c536d74820ab",
   "metadata": {},
   "source": [
    "The terms in the ELBO are expectations and are typcially computed using Monte Carlo estimates.\n",
    "\n",
    "Unfortunately, optimizing the ELBO using the terms we just derived might be suboptimal: The consistency term is computed as an expectation over two random variables\n",
    "${x_{t−1}, x_{t+1}}$ for every timestep and the variance of its Monte Carlo estimate could potentially be higher than a term that is estimated using only one random variable per timestep. As it is computed by summing up $T − 1$ consistency terms, the final estimated value of the ELBO may have high variance for large $T$ values.\n",
    "\n",
    "The key insight is that we can rewrite encoder transitions as $q(x_{t}|x_{t−1}) = q(x_t|x_{t−1}, x_0)$, where the extra conditioning term is superfluous due to the Markov property. Then, according to Bayes rule, we can rewrite each transition as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    q(x_t|x_{t-1}, x_0)  = \\frac{q(x_{t-1}|x_t, x_0) q(x_t|x_0)}{q(x_{t-1}|x_0)}\n",
    "\\end{equation*}\n",
    "\n",
    "Retry the derivation of the ELBO using this equation and show that it can be written as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log p(x_0) \\geq \\underbrace{\\mathbb{E}_{q(x_1|x_0)}[\\log p_{\\theta}(x_0|x_1)]}_{\\text{reconstruction term}}\n",
    "- \\underbrace{D_{KL}(q(x_{T}|x_{0}) || p(x_{T}))}_{\\text{prior matching term}}\n",
    "- \\sum_{t=2}^{T} \\underbrace{\\mathbb{E}_{q(x_{t}|x_0)} [D_{KL} (q(x_{t-1}|x_{t},x_0) || p_{\\theta}(x_{t-1}|x_{t}))]}_{\\text{denoising matching term}}\n",
    "\\end{equation*}\n",
    "\n",
    "Again, provide interpretations for the terms and argue if and why this is a better or worse optimization target than the previous ELBO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa54a9-b3b2-43c2-96e9-9f0c934d80a6",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee5eae-f7f9-41f5-8828-ea6489faec82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise 4: Backward Diffusion Process (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d720bf-acf1-45d2-b7ae-3ac49878c127",
   "metadata": {},
   "source": [
    "To instantiate this objective we need to determine the distribution $q(x_{t-1} \\mid x_t, x_0)$, which can be thought of as the backward diffusion process.\n",
    "\n",
    "We know that $q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1}, (1-\\alpha_t) \\textbf{I})$\n",
    "\n",
    "\n",
    "Show that \n",
    "\\begin{align*}\n",
    "    q(x_{t-1} \\mid x_t, x_0) = \\mathcal N\\left(x_{t-1}; \\frac1{1-\\alpha^t} \\left( \\sqrt \\alpha (1-\\alpha^{t-1}) x_t + \\sqrt{\\alpha^{t-1}} (1-\\alpha) x_0 \\right), \\frac{(1-\\alpha)(1-\\alpha^{t-1})}{1-\\alpha^t}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "Assume constant $\\alpha = \\alpha_t$  $\\forall t \\in [0, T]$.\n",
    "\n",
    "*Hint: Use the result for Exercise 1b) and the conditional version of Bayes' theorem*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64300d8-f8b2-4315-b864-b07ff35ea04b",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c33c8-c9e4-4d4c-af2b-c59b691e47a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise 5: Training (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c8102b-70e6-4c66-881f-f644910fc987",
   "metadata": {},
   "source": [
    "Now that we know $q(x_{t-1} \\mid x_t, x_0)$, we can construct $p_\\theta(x_{t-1} \\mid x_t)$ by letting our model predict $x_0$ and plugging that prediction into the solution for $q(x_{t-1} \\mid x_t, x_0)$. \n",
    "\n",
    "Since both $q(x_{t-1} \\mid x_t, x_0)$ and $p_\\theta(x_{t-1} \\mid x_t)$ are Gaussian, maximizing the ELBO results in minimizing the mean-squared error. \n",
    "\n",
    "Implement a training loop for the network specified below. \n",
    "\n",
    "Do a uniform Monte-Carlo estimate over $t$, i.e., for each sample, draw a time step $t$ uniformly from ${1, \\dots, T}$ and corrupt the image accordingly to obtain $x_t$. \n",
    "Then train the VDM to predict $\\varepsilon$ from $x_t$, i.e. minimize the MSE between the ground truth image $x_0$ and the predicted image $\\hat{x}_0$.\n",
    "\n",
    "For efficiency reasons, we share the model parameters over time. \n",
    "To improve performance, we feed $t$ as an additional input alongside $x_t$. \n",
    "The provided hyperparameters should work to get \"ok\" results but feel free to experiment with them as always. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e826df4-675c-45f8-9a62-53c0a5e81cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "lr = 1e-3\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=tfm.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=tfm.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "class VDM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, 4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)\n",
    "        self.debn1 = nn.BatchNorm2d(128)\n",
    "        self.deconv2 = nn.ConvTranspose2d(256, 64, 4, stride=2, padding=1)\n",
    "        self.debn2 = nn.BatchNorm2d(64)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 32, 4, stride=2, padding=1)\n",
    "        self.debn3 = nn.BatchNorm2d(32)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1)\n",
    "        self.debn4 = nn.BatchNorm2d(3)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = torch.ones_like(x[:, :1, :, :]) * (t/T * 2 - 1)\n",
    "        x0 = torch.cat([x, t], 1)\n",
    "        x1 = F.relu(self.bn1(self.conv1(x0)))\n",
    "        x2 = F.relu(self.bn2(self.conv2(x1)))\n",
    "        x3 = F.relu(self.bn3(self.conv3(x2)))\n",
    "        x4 = F.relu(self.bn4(self.conv4(x3)))\n",
    "        x4 = F.relu(self.debn1(self.deconv1(x4)))\n",
    "        x3 = F.relu(self.debn2(self.deconv2(torch.cat([x4, x3], 1))))\n",
    "        x2 = F.relu(self.debn3(self.deconv3(torch.cat([x3, x2], 1))))\n",
    "        return F.tanh(self.debn4(self.deconv4(torch.cat([x2, x1], 1))))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "vdm = VDM().to(device)\n",
    "vdm.train()\n",
    "optimizer = torch.optim.Adam(vdm.parameters(), lr=lr)\n",
    "print(torch.nn.utils.parameters_to_vector(vdm.parameters()).numel(), 'parameters')\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cbc395-80c5-4946-90bb-fa4435316651",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bonus Exercise: Inference and Visualization (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574b888-3b46-412a-850c-711b5fd5a722",
   "metadata": {},
   "source": [
    "The key idea of diffusion models is to generate images by performing the backward diffusion process using our trained model to predict $x_0$. \n",
    "That is, at time $t$, sample $x_{t-1}$ from  \n",
    "\\begin{align*}\n",
    "    p(x_{t-1} \\mid x_t, \\hat x_0) = \\mathcal N\\left(\\frac1{1-\\alpha^t} \\left( \\sqrt \\alpha (1-\\alpha^{t-1}) x_t + \\sqrt{\\alpha^{t-1}} (1-\\alpha) \\hat x_0 \\right), \\frac{(1-\\alpha)(1-\\alpha^{t-1})}{1-\\alpha^t} I\\right)\n",
    "\\end{align*}\n",
    "where $\\hat x_0$ is the model prediction from $x_t$. \n",
    "\n",
    "Iterate this scheme from $t=T$ until $t=0$ to generate a bunch of sample images.\n",
    "\n",
    "Visualize the denoising process of going from pure noise to a sample image (inverse of exercise 1a). Plot either a sequence of images or make a short animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2e2f6-4afd-447c-ab63-281e1ba3890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
